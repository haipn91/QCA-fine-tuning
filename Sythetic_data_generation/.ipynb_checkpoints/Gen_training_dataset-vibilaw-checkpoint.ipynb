{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce9322-ed5a-447e-bc8e-a84d2f19d3ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]= \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "import nest_asyncio\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8088b28-2f38-40a1-bfc5-712f73b3b753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # Insert key for training data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd61c22-edeb-4a93-b1a4-49e7f962bb08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.schema import MetadataMode\n",
    "import re\n",
    "import uuid\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "TRAIN_FILES = \"Raw_data/vibilaw/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d9c1b-a13e-49e6-890a-0ad320bf9df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "def load_corpus(directory, verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Loading files in {directory}\")\n",
    "\n",
    "    reader = SimpleDirectoryReader(directory)\n",
    "    docs = reader.load_data()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "    parser = SimpleNodeParser.from_defaults()\n",
    "    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Parsed {len(nodes)} nodes\")\n",
    "    corpus = {node.node_id: node.get_content(metadata_mode=MetadataMode.NONE) for node in nodes}\n",
    "    return nodes, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55c05d-8220-408f-9a74-514fe5349a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_nodes, train_corpus = load_corpus(TRAIN_FILES, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d944bfe-1ea5-4dea-80af-487544a81bd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for relationship, related_node_info in train_nodes[0].relationships.items():\n",
    "    if str(relationship) == \"NodeRelationship.SOURCE\":\n",
    "        node_id = related_node_info.node_id\n",
    "        text_content = val_nodes[0].text\n",
    "        print(f\"node_id: {node_id}\")\n",
    "        print(f\"text: {text_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f559a20b-8144-49e8-8904-ed1278c488c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(\n",
    "    nodes,\n",
    "    num_questions_per_chunk=5,\n",
    "    prompt_template=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Automatically generate hypothetical questions that could be answered with\n",
    "    doc in the corpus.\n",
    "    \"\"\"\n",
    "    llm = OpenAI(model='gpt-3.5-turbo-16k')\n",
    "\n",
    "    prompt_template = prompt_template or \"\"\"\\\n",
    "    Cho thông tin về một điều luật bên dưới.\\n\n",
    "    ----------------------\n",
    "    {context_str}\n",
    "    ------------- ---------\\\n",
    "    Với thông tin đã cho chứ không phải kiến thức có sẵn.\n",
    "    Bạn là Giáo viên/Giáo sư về luật pháp Việt Nam. \n",
    "    Nhiệm vụ của bạn là thiết lập {num_questions_per_chunk} bộ câu hỏi và đáp án cho bài kiểm tra/bài kiểm tra sắp tới.\n",
    "    Các câu hỏi nên đa dạng về bản chất trong toàn bộ tài liệu và bắt đầu bằng \"Câu hỏi:\".\n",
    "    Giới hạn các câu hỏi trong phạm vi thông tin ngữ cảnh được cung cấp.\n",
    "    Đáp án phải bao gồm căn cứ trả lời câu hỏi là Khoản mấy, Điều mấy và văn bản luật nào.\n",
    "    Đáp án bắt đầu bằng \"Đáp án:\".\n",
    "    \"\"\"\n",
    "\n",
    "    queries = {}\n",
    "    answers = {}\n",
    "    corpus = {}\n",
    "    relevant_docs = {}\n",
    "        \n",
    "    for relationship, related_node_info in nodes.relationships.items():\n",
    "        if str(relationship) == \"NodeRelationship.SOURCE\":\n",
    "            node_id = related_node_info.node_id\n",
    "            context_text = nodes.text\n",
    "            query = prompt_template.format(context_str=context_text, num_questions_per_chunk=num_questions_per_chunk)\n",
    "            response = llm.complete(query)\n",
    "\n",
    "            result = str(response).strip().split(\"\\n\")\n",
    "            result = [\n",
    "                re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n",
    "            ]\n",
    "            result = [result for result in result if len(result) > 0]\n",
    "            question = []\n",
    "            answer = []\n",
    "            print(\"Generating data at node\", node_id)\n",
    "            for text in result:\n",
    "                if text.startswith('Câu hỏi'):\n",
    "                    question.append(text.split(':', 1)[1].strip())\n",
    "                else:\n",
    "                    split_item = text.split(':', 1)\n",
    "                    answer_ = ':'.join(split_item[1:]).strip()\n",
    "                    answer.append(answer_)\n",
    "                    #answer = text.split(':', 1)[1:].strip()\n",
    "            for q, a in zip(question, answer):\n",
    "                question_id = str(uuid.uuid4())  \n",
    "                queries[question_id] = q\n",
    "                answers[question_id] = a\n",
    "                corpus[node_id] = context_text\n",
    "                relevant_docs[question_id] = [node_id]\n",
    "              \n",
    "    return queries, relevant_docs, answers, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60b03e-3eba-4b08-8b20-195f3d0d2cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_queries, train_relevant_docs, train_answers, train_corpus = {}, {}, {}, {}\n",
    "for index, item in enumerate(train_nodes):\n",
    "    if index % 50 == 0:\n",
    "        print(\"Index:\", index)\n",
    "    try:\n",
    "        train_query, train_relevant_doc, train_answer, train_cp = generate_queries(item)\n",
    "        train_queries.update(train_query)\n",
    "        train_answers.update(train_answer)\n",
    "        train_corpus.update(train_cp)\n",
    "        train_relevant_docs.update(train_relevant_doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing item {index}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d5ca5-738e-4cfb-982b-a1465f4a4bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DATASET_FPATH = 'QAC_fine_tuning/data_rag/vibilaw/train_dataset.json'\n",
    "train_dataset = {\n",
    "    'queries': train_queries,\n",
    "    'answers': train_answers,\n",
    "    'corpus': train_corpus,\n",
    "    'relevant_docs': train_relevant_docs\n",
    "}\n",
    "with open(TRAIN_DATASET_FPATH, 'w+') as f:\n",
    "    json.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a786d5-5ca4-41d5-894d-a3f792a17003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_invalid_answers(dataset):\n",
    "    # Danh sách các id không có giá trị\n",
    "    invalid_ids = []\n",
    "    for id_, answer in dataset['answers'].items():\n",
    "        # Kiểm tra xem câu trả lời có giá trị không\n",
    "        if not answer.strip():\n",
    "            invalid_ids.append(id_)\n",
    "        if \"Không có thông tin cụ thể về việc này trong đoạn văn trên\" in answer:\n",
    "            invalid_ids.append(id_)\n",
    "            print(id_)\n",
    "    \n",
    "    # Loại bỏ các id không có giá trị\n",
    "    for id_ in invalid_ids:\n",
    "        # Loại bỏ câu hỏi tương ứng\n",
    "        if id_ in dataset['queries']:\n",
    "            del dataset['queries'][id_]\n",
    "        # Loại bỏ câu trả lời không có giá trị\n",
    "        del dataset['answers'][id_]\n",
    "        # Loại bỏ văn bản liên quan tương ứng\n",
    "        if id_ in dataset['relevant_docs']:\n",
    "            del dataset['relevant_docs'][id_]\n",
    "    \n",
    "    return dataset\n",
    "# Sử dụng hàm để loại bỏ các câu trả lời không có giá trị\n",
    "with open(TRAIN_DATASET_FPATH, 'r', encoding='utf-8') as json_file:\n",
    "    train_dataset = json.load(json_file)\n",
    "\n",
    "clean_train_dataset = remove_invalid_answers(train_dataset)\n",
    "with open(TRAIN_DATASET_FPATH, 'w+') as f:\n",
    "    json.dump(clean_train_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
