{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dce9322-ed5a-447e-bc8e-a84d2f19d3ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]= \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdba221d-7723-4efe-942b-ee3a097b8337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8088b28-2f38-40a1-bfc5-712f73b3b753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" #Insert key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dd61c22-edeb-4a93-b1a4-49e7f962bb08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.schema import MetadataMode\n",
    "import re\n",
    "import uuid\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "TRAIN_FILES = \"./Raw_data/zalo_2021/train_val_dataset/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "684d9c1b-a13e-49e6-890a-0ad320bf9df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_txt_files(folder_path):\n",
    "    text_list = []\n",
    "    # Lặp qua tất cả các file trong thư mục\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                text_list.append(text)\n",
    "    return text_list\n",
    "\n",
    "# Đường dẫn tới thư mục chứa các file .txt\n",
    "train_list = load_txt_files(TRAIN_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f559a20b-8144-49e8-8904-ed1278c488c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_queries(\n",
    "    text,\n",
    "    num_questions_per_chunk=1,\n",
    "    prompt_template=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Automatically generate hypothetical questions, and answers that could be answered with\n",
    "    doc in the corpus.\n",
    "    \"\"\"\n",
    "    llm = OpenAI(model='gpt-3.5-turbo-16k')\n",
    "\n",
    "    prompt_template = prompt_template or \"\"\"\\\n",
    "    Cho thông tin về một điều luật bên dưới.\\n\n",
    "    ----------------------\n",
    "    {context_str}\n",
    "    ------------- ---------\\\n",
    "    Với thông tin đã cho chứ không phải kiến thức có sẵn.\n",
    "    Bạn là Giáo viên/Giáo sư về luật pháp Việt Nam. \n",
    "    Nhiệm vụ của bạn là thiết lập {num_questions_per_chunk} bộ câu hỏi và đáp án cho bài kiểm tra/bài kiểm tra sắp tới.\n",
    "    Các câu hỏi nên đa dạng về bản chất trong toàn bộ tài liệu và bắt đầu bằng \"Câu hỏi:\".\n",
    "    Giới hạn các câu hỏi trong phạm vi thông tin ngữ cảnh được cung cấp.\n",
    "    Đáp án phải bao gồm căn cứ trả lời câu hỏi là Khoản mấy, Điều mấy và văn bản luật nào.\n",
    "    Đáp án bắt đầu bằng \"Đáp án:\".\n",
    "    \"\"\"\n",
    "\n",
    "    queries = {}\n",
    "    answers = {}\n",
    "    corpus = {}\n",
    "    relevant_docs = {}\n",
    "        \n",
    "\n",
    "    context_text = text\n",
    "    query = prompt_template.format(context_str=context_text, num_questions_per_chunk=num_questions_per_chunk)\n",
    "    response = llm.complete(query)\n",
    "\n",
    "    result = str(response).strip().split(\"\\n\")\n",
    "    result = [\n",
    "        re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n",
    "    ]\n",
    "    result = [result for result in result if len(result) > 0]\n",
    "\n",
    "    question = []\n",
    "    answer = []\n",
    "\n",
    "    for text in result:\n",
    "        if text.startswith('Câu hỏi'):\n",
    "            question.append(text.split(':', 1)[1].strip())\n",
    "        else:\n",
    "            split_item = text.split(':', 1)\n",
    "            answer_ = ':'.join(split_item[1:]).strip()\n",
    "            answer.append(answer_)\n",
    "            #answer = text.split(':', 1)[1:].strip()\n",
    "    for q, a in zip(question, answer):\n",
    "        question_id = str(uuid.uuid4())\n",
    "        node_id = str(uuid.uuid4())\n",
    "        queries[question_id] = q\n",
    "        answers[question_id] = a\n",
    "        corpus[node_id] = context_text\n",
    "        relevant_docs[question_id] = [node_id]\n",
    "               \n",
    "    return queries, relevant_docs, answers, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60b03e-3eba-4b08-8b20-195f3d0d2cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_queries, train_relevant_docs, train_answers, train_corpus = {}, {}, {}, {}\n",
    "for index, item in enumerate(train_list):\n",
    "    if index % 50 == 0:\n",
    "        print(\"Index:\", index)\n",
    "    try:\n",
    "        train_query, train_relevant_doc, train_answer, train_cp = generate_queries(item)\n",
    "        train_queries.update(train_query)\n",
    "        train_answers.update(train_answer)\n",
    "        train_corpus.update(train_cp)\n",
    "        train_relevant_docs.update(train_relevant_doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing item {index}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "266ec14e-56a9-4687-bdb1-29dfbf6d740c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DATASET_FPATH = '../data_rag/zalo2020/train_dataset.json'\n",
    "train_dataset = {\n",
    "    'queries': train_queries,\n",
    "    'answers': train_answers,\n",
    "    'corpus': train_corpus,\n",
    "    'relevant_docs': train_relevant_docs\n",
    "}\n",
    "with open(TRAIN_DATASET_FPATH, 'w+') as f:\n",
    "    json.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5a786d5-5ca4-41d5-894d-a3f792a17003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_invalid_answers(dataset):\n",
    "    # Danh sách các id không có giá trị\n",
    "    invalid_ids = []\n",
    "    for id_, answer in dataset['answers'].items():\n",
    "        # Kiểm tra xem câu trả lời có giá trị không\n",
    "        if not answer.strip():\n",
    "            invalid_ids.append(id_)\n",
    "        if \"Không có thông tin cụ thể về việc này trong đoạn văn trên\" in answer:\n",
    "            invalid_ids.append(id_)\n",
    "            print(id_)\n",
    "    \n",
    "    # Loại bỏ các id không có giá trị\n",
    "    for id_ in invalid_ids:\n",
    "        # Loại bỏ câu hỏi tương ứng\n",
    "        if id_ in dataset['queries']:\n",
    "            del dataset['queries'][id_]\n",
    "        # Loại bỏ câu trả lời không có giá trị\n",
    "        del dataset['answers'][id_]\n",
    "        # Loại bỏ văn bản liên quan tương ứng\n",
    "        if id_ in dataset['relevant_docs']:\n",
    "            del dataset['relevant_docs'][id_]\n",
    "    \n",
    "    return dataset\n",
    "# Sử dụng hàm để loại bỏ các câu trả lời không có giá trị\n",
    "with open(TRAIN_DATASET_FPATH, 'r', encoding='utf-8') as json_file:\n",
    "    train_dataset = json.load(json_file)\n",
    "\n",
    "clean_train_dataset = remove_invalid_answers(train_dataset)\n",
    "with open(TRAIN_DATASET_FPATH, 'w+') as f:\n",
    "    json.dump(clean_train_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
